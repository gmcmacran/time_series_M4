---
format:
  gfm:
    html-math-method: webtex
jupyter: python3
---

# Summary

This repo takes the [M4](https://www.sciencedirect.com/science/article/pii/S0169207019301128) competition's data and builds 7 different models:

-   7th: Neural Basis Expansion Analysis with Exogenous (NBEATSx)
-   10th: Neural Hierarchical Interpolation for Time Series (NHITS)
-   34th: Temporal Fusion Transformer (TFT)
-   34th: Ridge Regression
-   34th: Boosting
-   37th: Seasonal Naive
-   41st: Last Value Naive

My best model beat the 7th place submission. My second best model beat the 10th place submission.

# Data Overview

Beginning in 1982, Spyros Makridakis (the M in M4) has been running time series competitions that define the state of the art in time series modeling.

-   M1: 111 time series in 1982
-   M2: 29 series in 1993
-   M3: 300,3 in 2000
-   M4: 100,000 in 2020
-   M5: 42,840 in 2021
-   M6: 50 series in 2022

This repo focuses on the M4 competition as it includes the most series and poses the highest computational challenge. The 100,000 time series are broken up between hourly, daily, weekly, monthly, quarterly, and yearly datasets. Within each dataset, series come from different domains including economics, finance, demographics, and finance.

Data: 

-   Spyros Makridaki's [website](https://forecasters.org/resources/time-series-data/)
-   [Github](https://github.com/Mcompetitions)

### Cleaning Process
Pulling the raw data from the competition page lead to challenging data quality issues. First, series Y13190 contained 835 years worth of data meaning data collection started before the printing press was created. Second, series Y3820 ends in the future. I am not the only person to notice [this](https://openforecast.org/2020/03/01/m-competitions-from-m4-to-m5-reservations-and-expectations/). I was unable to find clear documentation on how participates handled these series. It is possible they simply accepted these series as is.

Due to these challenges, I avoid cleaning the data myself. Instead, I leverage Nixtla's dataset [package](https://nixtlaverse.nixtla.io/datasetsforecast/index.html) which includes a cleaned version of data. I only confirm each dataset has the correct number of series.

-   Step 1: Download data.
-   Step 2: Count the number of series.
-   Step 3: Write data.

### Data Exploration

```{python}
#| echo: false
import os
from functools import reduce

import numpy as np
import pandas as pd
from plotnine import (aes, coord_flip, geom_boxplot, geom_col, geom_hline,
                      ggplot, ggsave, labs, scale_y_continuous)
from mizani.formatters import comma_format

os.chdir("S:\Python\projects\exploration")

def load_data(data_type):
    if data_type == "hourly":
        train_file = os.path.join(os.getcwd(), "data\\hourly-train.csv")
    elif data_type == "daily":
        train_file = os.path.join(os.getcwd(), "data\\daily-train.csv")
    elif data_type == "weekly":
        train_file = os.path.join(os.getcwd(), "data\\weekly-train.csv")
    elif data_type == "monthly":
        train_file = os.path.join(os.getcwd(), "data\\monthly-train.csv")
    elif data_type == "quarterly":
        train_file = os.path.join(os.getcwd(), "data\\quarterly-train.csv")
    elif data_type == "yearly":
        train_file = os.path.join(os.getcwd(), "data\\yearly-train.csv")

    train = pd.read_csv(train_file)
    train['data'] = data_type

    return train

train_data = map(load_data, ["hourly", "daily", "weekly", "monthly", "quarterly", "yearly"])
train_data = reduce(lambda x, y: pd.concat([x, y]), train_data)

graph_data = (
    train_data.
    groupby(['data', 'unique_id'], as_index=False).
    agg(
        row_count = ('ds', 'count'),
        avg_value = ('y', 'mean'),
        std_value = ('y', 'std')
    )
)
graph_data["data"] = graph_data["data"].astype("category")
graph_data["data"] = graph_data["data"].cat.reorder_categories(
    ["hourly", "daily", "weekly", "monthly", "quarterly", "yearly"]
)
```

Long frequency data (i.e. yearly) tend to be shorter series than short frequency series (i.e. daily). Within each dataset, the length of the series is inconsistent.

```{python}
#| echo: false
(
    ggplot(graph_data, aes(x="data", y="row_count"))
    + geom_boxplot(alpha=0.40)
    + labs(
        x="Data",
        y="Lengths Of Individual Series",
    )
    + scale_y_continuous(labels = comma_format())
    + coord_flip()
)
```

There is large variability in average value of series in most datasets.

```{python}
#| echo: false
(
    ggplot(graph_data, aes(x="data", y="avg_value"))
    + geom_boxplot(alpha=0.40)
    + labs(
        x="Data",
        y="Average Values Of Individual Series",
    )
    + scale_y_continuous(labels = comma_format())
    + coord_flip(ylim=[0, 10000])
)
```

Within each dataset, there is large variability in standard deviations of series. 

```{python}
#| echo: false
(
    ggplot(graph_data, aes(x="data", y="std_value"))
    + geom_boxplot(alpha=0.40)
    + labs(
        x="Data",
        y="Standard Deviations Of Individual Series",
    )
    + scale_y_continuous(labels = comma_format())
    + coord_flip(ylim=[0, 20000])
)
```

# Ecosystem Overview
The Nixtla ecosystem defines a standardized data shape and provides well over 50 different models with a unified interface. This makes changing between time series models as easy as changing between regression models with sci-kit learn. Cross validation and a wide variety of metrics are provided as well.

### Neural Forecast
Neural forecast is a collection of 18 different deep learning models specifically focused on time series analysis. They span many categories including recurrent neural networks, multilayer perceptron, transformers, and convolutional neural networks. All models are implemented with pytorch lightning meaning all models can run on a CPU, a GPU, or multiple GPUs.

### ML Forecast
ML forecast does not provide new models directly. Instead it provides functions to take the standardized time series data and shape into tabular data. From there, standard machine learning models are used. Models can come from sci-kit learn, xgboost, or any other library that follows a sci-kit learn interface.

# Model Summary

### Comparison with Other Participates 

The first vertical line is 1st place. The second vertical line is 7th place. Out of 61 submissions, my best model beat the 7th place model in the M4 competition.

```{python}
#| echo: false
fn = os.path.join("data", "metrics_df.csv")
fn = os.path.join(os.getcwd(), fn)
metric_df = pd.read_csv(fn)
metric_df["data"] = metric_df["data"].astype("category")
metric_df["data"] = metric_df["data"].cat.reorder_categories(
    ["hourly", "daily", "weekly", "monthly", "quarterly", "yearly"]
)
del fn

temp = metric_df.groupby("model", as_index=False)["smape"].mean()
(
    ggplot(temp, aes(x="model", y="smape"))
    + geom_col(alpha=0.40)
    + labs(title="Model Performance", x="Model", y="Average Smape")
    + geom_hline(yintercept=0.11374) # 1st
    + geom_hline(yintercept=0.12020) # 7th
    + scale_y_continuous(breaks=np.arange(0, 0.16, 0.02))
    + coord_flip()
)
```

### Naive Models
I was able to reproduce the smape for both the last value naive model and the seasonal naive model.

### ML Models
Ridge regression with sci-kit learn and boosting with lightgbm are the machine learning models used here. Their parameters were tuned using a time based cross validation. 50 different combinations of hyper parameters were done. Interestingly, boosting did no better than ridge regression. 

Roughly, training a single boosted model takes longer than training 50 ridge regression models.

### Deep Learning Models
All deep learning models are tuned on 50 sets of parameters.  Overall, deep learning is superior to machine learning based approaches for these data. NBeats and NHits beat boosting and ridge regression by a wide margin. The TFT model matches performance.

# Code Overview
Run order:

-   prep_data
-   train models (train_baseline_models, train_deep_lerning_models, train_ml_models)
-   summarize_results

train_stats_models was not used because code took too long to run.

Two conda environments were used. Most code runs on nixtlaEnv. The nixtlaEnvDeep environment was created to get GPU support for deep learning models. Conda commands and environment files are included.